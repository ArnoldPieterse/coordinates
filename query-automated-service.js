#!/usr/bin/env node

/**
 * Query Automated Prompt Service
 * Check performance and status of the automated prompt service
 */

import { fileURLToPath } from 'url';
import { dirname, join } from 'path';
import AutomatedPromptService from './src/ai/AutomatedPromptService.js';

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

class AutomatedServiceQuery {
    constructor() {
        this.service = null;
        this.config = {
            llmUrl: 'http://10.3.129.26:1234',
            queueSize: 5,
            maxRetries: 3,
            timeout: 30000,
            performanceThreshold: 0.8
        };
    }

    /**
     * Query the automated prompt service
     */
    async queryService() {
        console.log('üîç QUERYING AUTOMATED PROMPT SERVICE');
        console.log('====================================');
        console.log('Checking service performance and status...\n');

        try {
            // Initialize the service
            this.service = new AutomatedPromptService(this.config);
            
            // Wait for service to be ready
            await this.waitForServiceReady();
            
            // Query service status
            await this.queryServiceStatus();
            
            // Query performance metrics
            await this.queryPerformanceMetrics();
            
            // Query LLM status
            await this.queryLLMStatus();
            
            // Query queue status
            await this.queryQueueStatus();
            
            // Generate comprehensive report
            await this.generateComprehensiveReport();
            
        } catch (error) {
            console.error('‚ùå Failed to query service:', error);
            process.exit(1);
        }
    }

    /**
     * Wait for service to be ready
     */
    async waitForServiceReady() {
        console.log('‚è≥ Waiting for service to be ready...');
        
        let attempts = 0;
        const maxAttempts = 30;
        
        while (attempts < maxAttempts) {
            const status = this.service.getStatus();
            
            if (status.isRunning && status.llmStatus === 'available') {
                console.log('‚úÖ Service ready for querying');
                return;
            } else if (status.llmStatus === 'unavailable') {
                console.log('‚ùå LLM unavailable, retrying...');
            }
            
            await this.delay(2000);
            attempts++;
        }
        
        throw new Error('Service initialization timeout');
    }

    /**
     * Query service status
     */
    async queryServiceStatus() {
        console.log('\nüìä SERVICE STATUS QUERY');
        console.log('=======================');
        
        const status = this.service.getStatus();
        
        console.log(`Service Running: ${status.isRunning ? 'üü¢ Yes' : 'üî¥ No'}`);
        console.log(`LLM Status: ${status.llmStatus === 'available' ? 'üü¢ Available' : 'üî¥ Unavailable'}`);
        console.log(`Current Model: ${status.currentModel || 'Unknown'}`);
        console.log(`LLM URL: ${status.config.llmUrl}`);
        console.log(`Queue Size: ${status.queueSize}/${status.config.queueSize}`);
        console.log(`Processing: ${status.processingSize}`);
        console.log(`Completed: ${status.completedSize}`);
    }

    /**
     * Query performance metrics
     */
    async queryPerformanceMetrics() {
        console.log('\nüìà PERFORMANCE METRICS QUERY');
        console.log('=============================');
        
        const status = this.service.getStatus();
        const performance = status.performance;
        
        if (performance.totalPrompts > 0) {
            const successRate = (performance.successfulPrompts / performance.totalPrompts * 100).toFixed(2);
            const failureRate = (performance.failedPrompts / performance.totalPrompts * 100).toFixed(2);
            
            console.log(`Total Prompts: ${performance.totalPrompts}`);
            console.log(`Successful: ${performance.successfulPrompts}`);
            console.log(`Failed: ${performance.failedPrompts}`);
            console.log(`Success Rate: ${successRate}%`);
            console.log(`Failure Rate: ${failureRate}%`);
            console.log(`Average Response Time: ${performance.averageResponseTime.toFixed(2)}ms`);
            console.log(`Total Response Time: ${performance.totalResponseTime}ms`);
            
            // Performance assessment
            if (successRate >= 90) {
                console.log('üéØ Performance: üü¢ Excellent (>90% success rate)');
            } else if (successRate >= 80) {
                console.log('üéØ Performance: üü° Good (80-90% success rate)');
            } else {
                console.log('üéØ Performance: üî¥ Needs Improvement (<80% success rate)');
            }
            
            if (performance.averageResponseTime < 15000) {
                console.log('‚ö° Speed: üü¢ Fast (<15s average)');
            } else if (performance.averageResponseTime < 30000) {
                console.log('‚ö° Speed: üü° Acceptable (15-30s average)');
            } else {
                console.log('‚ö° Speed: üî¥ Slow (>30s average)');
            }
        } else {
            console.log('üìä No performance data available yet');
        }
    }

    /**
     * Query LLM status
     */
    async queryLLMStatus() {
        console.log('\nü§ñ LLM STATUS QUERY');
        console.log('===================');
        
        try {
            const controller = new AbortController();
            const timeoutId = setTimeout(() => controller.abort(), 10000);
            
            const response = await fetch(`${this.config.llmUrl}/models`, {
                method: 'GET',
                signal: controller.signal
            });
            
            clearTimeout(timeoutId);
            
            if (response.ok) {
                const models = await response.json();
                console.log(`üü¢ LLM Available at ${this.config.llmUrl}`);
                console.log(`üìã Available Models: ${models.data?.length || 0}`);
                
                if (models.data && models.data.length > 0) {
                    console.log('üìù Model Details:');
                    models.data.forEach((model, index) => {
                        console.log(`  ${index + 1}. ${model.id} (${model.object})`);
                    });
                }
            } else {
                console.log(`üî¥ LLM Error: ${response.status} ${response.statusText}`);
            }
        } catch (error) {
            console.log(`üî¥ LLM Unavailable: ${error.message}`);
        }
    }

    /**
     * Query queue status
     */
    async queryQueueStatus() {
        console.log('\nüìã QUEUE STATUS QUERY');
        console.log('=====================');
        
        const status = this.service.getStatus();
        
        console.log(`Queue Size: ${status.queueSize}/${status.config.queueSize}`);
        console.log(`Processing: ${status.processingSize}`);
        console.log(`Completed: ${status.completedSize}`);
        
        // Queue utilization
        const utilization = (status.queueSize / status.config.queueSize * 100).toFixed(2);
        console.log(`Queue Utilization: ${utilization}%`);
        
        if (utilization >= 80) {
            console.log('üìä Queue Status: üü¢ Well Utilized (>80%)');
        } else if (utilization >= 50) {
            console.log('üìä Queue Status: üü° Moderate Utilization (50-80%)');
        } else {
            console.log('üìä Queue Status: üî¥ Low Utilization (<50%)');
        }
    }

    /**
     * Generate comprehensive report
     */
    async generateComprehensiveReport() {
        console.log('\nüìã COMPREHENSIVE SERVICE REPORT');
        console.log('===============================');
        
        const status = this.service.getStatus();
        const performance = status.performance;
        
        // Overall health score
        let healthScore = 0;
        let healthFactors = [];
        
        // Service running
        if (status.isRunning) {
            healthScore += 25;
            healthFactors.push('‚úÖ Service Running');
        } else {
            healthFactors.push('‚ùå Service Not Running');
        }
        
        // LLM available
        if (status.llmStatus === 'available') {
            healthScore += 25;
            healthFactors.push('‚úÖ LLM Available');
        } else {
            healthFactors.push('‚ùå LLM Unavailable');
        }
        
        // Queue utilization
        const utilization = status.queueSize / status.config.queueSize;
        if (utilization >= 0.8) {
            healthScore += 25;
            healthFactors.push('‚úÖ Good Queue Utilization');
        } else if (utilization >= 0.5) {
            healthScore += 15;
            healthFactors.push('üü° Moderate Queue Utilization');
        } else {
            healthFactors.push('‚ùå Low Queue Utilization');
        }
        
        // Performance
        if (performance.totalPrompts > 0) {
            const successRate = performance.successfulPrompts / performance.totalPrompts;
            if (successRate >= 0.9) {
                healthScore += 25;
                healthFactors.push('‚úÖ Excellent Performance');
            } else if (successRate >= 0.8) {
                healthScore += 15;
                healthFactors.push('üü° Good Performance');
            } else {
                healthFactors.push('‚ùå Poor Performance');
            }
        } else {
            healthFactors.push('üü° No Performance Data Yet');
        }
        
        // Health assessment
        console.log(`üè• Overall Health Score: ${healthScore}/100`);
        
        if (healthScore >= 90) {
            console.log('üéØ Health Status: üü¢ Excellent');
        } else if (healthScore >= 75) {
            console.log('üéØ Health Status: üü° Good');
        } else if (healthScore >= 50) {
            console.log('üéØ Health Status: üü† Fair');
        } else {
            console.log('üéØ Health Status: üî¥ Poor');
        }
        
        console.log('\nüìä Health Factors:');
        healthFactors.forEach(factor => console.log(`  ${factor}`));
        
        // Recommendations
        console.log('\nüí° Recommendations:');
        if (healthScore < 90) {
            if (!status.isRunning) {
                console.log('  - Start the automated prompt service');
            }
            if (status.llmStatus !== 'available') {
                console.log('  - Check LLM availability at http://10.3.129.26:1234');
            }
            if (utilization < 0.8) {
                console.log('  - Increase prompt generation to improve queue utilization');
            }
            if (performance.totalPrompts > 0 && performance.successfulPrompts / performance.totalPrompts < 0.9) {
                console.log('  - Investigate and fix prompt failures');
            }
        } else {
            console.log('  - Service is performing excellently!');
            console.log('  - Consider scaling up for higher throughput');
        }
    }

    /**
     * Utility delay function
     */
    delay(ms) {
        return new Promise(resolve => setTimeout(resolve, ms));
    }
}

// Run the query if this script is executed directly
if (import.meta.url === `file://${process.argv[1]}`) {
    const query = new AutomatedServiceQuery();
    query.queryService().catch(error => {
        console.error('‚ùå Query failed:', error);
        process.exit(1);
    });
}

export default AutomatedServiceQuery; 